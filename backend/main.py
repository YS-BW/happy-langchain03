import asyncio
import json
from typing import AsyncGenerator
from backend.config.schema.chat import ChatRequest
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from dotenv import load_dotenv

from backend.core.agents.agent import agent
load_dotenv()

print("âœ… Agentic RAG Agent enabled")
print("ğŸ§  Agent decides when to use tools autonomously")

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["*"],
)


# @app.post("/chat/stream")

# async def chat_stream(request: ChatRequest):
#     """æµå¼èŠå¤©æ¥å£-updates"""
    
#     async def event_stream() -> AsyncGenerator[str, None]:
#         async for chunk in agent.astream(
#             {"messages": [{"role": "user", "content": request.question}]},
#             stream_mode="updates",
#             stream_subgraphs=True
#         ):
            
#             chunk_data = {
#                 "type": "chunk",
#                 "data": chunk,
#                 "timestamp": asyncio.get_event_loop().time()
#             }
#             yield f"data: {json.dumps(chunk_data, ensure_ascii=False, default=str)}\n\n"
  

#     return StreamingResponse(
#         event_stream(),
#         media_type="text/event-stream",  # âœ… SSEæ ‡å‡†æ ¼å¼
#         headers={
#             "Cache-Control": "no-cache",
#             "Connection": "keep-alive",
#             "X-Accel-Buffering": "no"
#         }
#     )
"""
  {
  "type": "chunk",
  "data": {
    "model": {
      "messages": [å•ä¸ªå®Œæ•´æ¶ˆæ¯å¯¹è±¡]
    }
  }
}
"""    
@app.post("/chat/messages")
async def chat_stream(request: ChatRequest):
    
    input_dict = {
        "messages": [msg.model_dump() for msg in request.messages]
    }
    
    # æ„å»ºé…ç½®å‚æ•°
    config_dict = {}
    if request.configurable:
        config_dict["configurable"] = request.configurable.model_dump(exclude_none=True)
    
    async def gen():
        # âœ… astream() è¿”å›å¼‚æ­¥è¿­ä»£å™¨
        async for token_chunk, metadata in agent.astream(
            input_dict,
            config=config_dict if config_dict else None,
            stream_mode="messages"
        ):
            node = metadata['langgraph_node']
            
            for block in token_chunk.content_blocks:
                if node == "model":
                    if block["type"] == "text":
                        yield f"data: {json.dumps({'text': block['text']})}\n\n"
        yield "data: [DONE]\n\n"
    
    return StreamingResponse(gen(), media_type="text/event-stream")
"""
{
  "type": "chunk", 
  "data": [
    "content='ä½ å¥½'...",  // å†…å®¹ç‰‡æ®µ
    {å…ƒæ•°æ®å¯¹è±¡}           // å…ƒæ•°æ®
  ]
}
"""



if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app)